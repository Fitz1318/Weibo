Python3环境下使用scrapy爬取新浪微博用户信息和微博信息，并存储到MongoDB中。

2018/4/11更新:\<br>
修改了部分代码，现在能够在爬取2W左右就被封IP了，\<br>
最关键的是内容爬取不完整！！！\<br>

--------------------*******************---------------------

2018/4/17更新：\<br>
①解决uid异常抛错处理\<br>
②针对爬虫过程中出现的个例403错误进行retry设置\<br>
③在pipelines中解决之前对于“昨天发表的微博的时间标准化问题“\<br>
  之前对于昨天发表的微博的时间一直不正确\<br>
④使用Github上的fake_useragent，替代之前少量的User-Agent\<br>
⑤增加了代码说明，方便阅读\<br>
⑥设置DOWNLOAD_DELAY = 0.5时是不会封号的，但是效率差点\<br>